<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Authorial analysis #1: Geometry - Jealous Markup</title>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,600,700|Bree+Serif&amp;subset=latin-ext" rel="stylesheet">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel='stylesheet' href='/_dev/jealous.css'>
</head>
<body>
  <div class="content-wrap">
    <div class="header-wrap">
      <div class="header">
        <a href="/"><img src="/static/jealous-markup-white.svg" alt="jealous markup" /></a>
        <div class="subhead">
          <div class="tagline">Software and language, mostly</div>
          <div class="links">
            <a href="/projects">Projects</a>
            <span> &bull; </span>
            <a href="https://twitter.com/twilliability" target="_blank" rel="nofollow noreferrer">Twitter</a>
            <span> &bull; </span>
            <a href="https://github.com/gugray" target="_blank" rel="nofollow noreferrer">Github</a>
          </div>
        </div>
      </div>
    </div>
    <div class="content">

      

<h1>Authorial analysis #1: Geometry</h1>
<p class="meta"><span class='date'>August 16, 2013</span>
<span class='filedunder'>
<span>NLP</span>
<span>Language</span>
<span>Software</span>
</span>
</p>

<p>
  If you missed the summer’s literary buzz: in mid-July the Sunday Times <a href="http://www.thetimes.co.uk/tto/arts/books/article3816183.ece" target="_blank" rel="noreferrer">revealed</a>  that the real author of the successful crime novel, <i>The Cuckoo’s Calling</i>, was indeed J.K. Rowling, and not Robert Galbraith, as the name on the cover said. What made the story particularly exciting for computer linguists was the use of “forensic stylometry” in verifying the author’s identity. On July 16, Ben Zimmer explained the science behind the analysis in his <a href="http://blogs.wsj.com/speakeasy/2013/07/16/the-science-that-uncovered-j-k-rowlings-literary-hocus-pocus/" target="_blank" rel="noreferrer">WSJ blog</a>, and soon enough a guest post by the language detective himself, Patrick Juola, followed on <a href="http://languagelog.ldc.upenn.edu/nll/?p=5315" target="_blank" rel="noreferrer">Language Log</a>. If that’s still not enough reading for you, there’s more on the subject in the <a href="http://chronicle.com/article/The-Professor-Who-Declared/140595/" target="_blank" rel="noreferrer">Chronicle of Higher Education</a>.
</p>
<p>
  I was a happy blog junkie with these articles for several mornings, but the story also challenged the DIY computer linguist in me. I wanted to try this myself, so I quickly cooked up my own authorial analyzer. To add a twist to the exercise, I decided to check how the same techniques would fare with a different language, Hungarian.
</p>

<h2>Quantifying variation</h2>
<p>
  Formidable as it may sound, forensic stylometry uses truly simple techniques of number-crunching to come up with an educated (and quantified) guess about a text’s author. It’s not a magic box that feeds on Word documents and comes up with the author’s name out of the blue. Its aim is to analyze a number of texts that are known to be written by, say, three different persons, and then to look at a new text and decide which of the three is the most likely author.
</p>
<p>
  It is the interplay of several things that makes algorithmic authorial analysis possible:
</p>
<ul>
  <li>
    The existence of linguistic variables – in very simple terms, the flexibility of every human language that allows us to
    express the same thing in several slightly different ways. In English, for instance, one might say <i>
      the glass is to the left
      the plate
    </i>, but another person may well say the glass is <i>on the left of the plate</i>
  </li>
  <li>The fact that some of these variables can unconsciously characterize an author while she is consciously producing text in a specific style, register, or even in an improvised archaic language. How much this appears to be true will be the subject an entire post later on.</li>
  <li>The possibility of capturing linguistic variables through “cheap,” low-tech textual metrics that yield rich data. Something as simple as spotting prepositions in a paragraph of text would already be too complicated and assume too much insight into language, but there are much more trivial, yet apparently effective options.</li>
</ul>
<p>
  So what do forensic stylometers count? I’ll discuss the simplest of measures here: word length. I’ll come back to some
  others at the end of the post. Word length, you’d think, is something very easy to measure, but a closer look does reveal
  a few potholes. For starters, linguists are notoriously at a loss to even define what a “word” is. So we’ll just skip the
  theory and say a word is whatever is separated by spaces in a paragraph of text. Having found our spaces, we trim periods,
  commas, quotes and other punctuation from the intervening characters, convert them all to lower case, and call what we’re
  left with a “word.” It may make sense to discard numbers at this point: they’re quite nasty because there are lots and lots
  of them and they all tend to be different. If you don’t mind wiring some knowledge specific to one language’s orthography
  into your code you may consider, for English, splitting <i>weren’t</i> into two and counting it as two words, <i>were</i> and <i>not</i>,
  or maybe <i>n’t</i>. But beware: you may well be losing some insight into an author’s style instead of producing cleaner
  data by doing it.
</p>
<p>
  Having now a whole bag of words in our hands, we could do something as simple as calculate the text’s average word length – that’s one single number. We’re much better off counting how many words with a length of 1, 2, 3 etc. letters the text contains. For English, the counting will stop somewhere around 30; anything longer than that is an outlier and we’ll just ignore it.
</p>
<p>
  At this point we have four series of numbers. In Author A’s texts we found 45 one-letter words, 93 two-letter words, 84 three-letter words, and so on, up to 32-letter words. This 32-number sequence is called a vector, and one in a 32-dimensional space at that. We have a similar vector for Author B, one for Author C, plus one for the text whose authorship we wish to determine. How can you quantify the similarity, or distance, of two vectors?
</p>

<h2>Measuring distance</h2>
<p>
  Nobody can imagine 32 dimensions; most of us, including myself, have trouble navigating three without accidents. Let’s stick with only 2 for now and look at the frequency of two-letter words and three-letter words. This gives us a pair of two-dimensional vectors whose distance we need to measure. Assuming we’ve counted occurrences from 1000 words of text from both authors:
</p>

<div class="centeredContent">
  <table class="styleA">
    <tr class="header">
      <td></td>
      <td>Count of two-letter words</td>
      <td>Count of three-letter words</td>
    </tr>
    <tr>
      <td class="rowHeader">Author A</td>
      <td>a<sub>x</sub> = 93</td>
      <td>a<sub>y</sub> = 84</td>
    </tr>
    <tr>
      <td class="rowHeader">Author B</td>
      <td>b<sub>x</sub> = 78</td>
      <td>b<sub>y</sub> = 108</td>
    </tr>
  </table>
</div>

<p>Let’s plot this on a graph:</p>
<p>
  <img src="/files/dnarr/auth01-fig01.png" alt="." width="502" height="365" />
</p>
<p>
  The obvious geometric distance formula, based on the Pythagorean Theorem about the length of a right-angled triangle’s sides, would give us:
</p>
<p class="formula">
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>d</mi>
    <mo>=</mo>
    <msqrt>
      <msup>
        <mfenced>
          <mrow>
            <msub>
              <mi>a</mi>
              <mi>x</mi>
            </msub>
            <mo>-</mo>
            <msub>
              <mi>b</mi>
              <mi>x</mi>
            </msub>
          </mrow>
        </mfenced>
        <mn>2</mn>
      </msup>
      <mo>+</mo>
      <msup>
        <mfenced>
          <mrow>
            <msub>
              <mi>a</mi>
              <mi>y</mi>
            </msub>
            <mo>-</mo>
            <msub>
              <mi>b</mi>
              <mi>y</mi>
            </msub>
          </mrow>
        </mfenced>
        <mn>2</mn>
      </msup>
    </msqrt>
  </math>
</p>

<p>
  With that formula, the “distance” between the two authors’ texts, as far as the distribution of two- and three-letter words goes, is about <i>d=20.3</i>. Nice, that’s a number, and we can say the unknown author’s text is closest to that author’s texts where this number is the smallest. There is, however, an important catch: we need to compare texts of equal length, but in reality texts come in all different sizes. If we take a sample from Author B that contains twice as many words, but the same proportion of two- and three-letter words, here’s what we get:
</p>

<div class="centeredContent">
  <table class="styleA">
    <tr class="header">
      <td></td>
      <td>Count of two-letter words</td>
      <td>Count of three-letter words</td>
    </tr>
    <tr>
      <td class="rowHeader">Author A</td>
      <td>a<sub>x</sub> = 93</td>
      <td>a<sub>y</sub> = 84</td>
    </tr>
    <tr>
      <td class="rowHeader">Author B</td>
      <td>b<sub>x</sub> = 156</td>
      <td>b<sub>y</sub> = 216</td>
    </tr>
  </table>
</div>

<p>
  <img src="/files/dnarr/auth01-fig03.png" alt="." width="502" height="365" />
</p>

<p>
  With our formula the distance is now about <i>d=146.26</i>, even though Author B has been meticulously producing
  the same proportion of two- and three-letter words, only in twice as much text. Clearly this is not what we need.
  We could refine our method by looking at the <i>relative</i> frequency of words instead: i.e., dividing every word
  count with the total number of words in the text. This gives us, practically, the <i>
    percentage
  </i>
  of two-letter words and three-letter words in the text at hand.
</p>

<div class="centeredContent">
  <table class="styleA">
    <tr class="header">
      <td></td>
      <td>Count of two-letter words</td>
      <td>Count of three-letter words</td>
    </tr>
    <tr>
      <td class="rowHeader">Author A</td>
      <td>a<sub>x</sub> = 9.3% (93 out of 1000)</td>
      <td>a<sub>y</sub> = 8.4% (84 out of 1000)</td>
    </tr>
    <tr>
      <td class="rowHeader">Author B</td>
      <td>b<sub>x</sub> = 7.8% (156 out of 2000)</td>
      <td>b<sub>y</sub> = 10.8% (216 out of 2000)</td>
    </tr>
  </table>
</div>

<p>
  <img src="/files/dnarr/auth01-fig04.png" alt="." width="502" height="365" />
</p>

<p>
  Our distance is now down to the nice approximate value of <i>d=20.3</i> again, even though we are comparing a smallish and a largish orange. Consequently, converting raw counts to relative frequencies is often the first thing to do in text analysis. But it turns out a different measure comes in even more handy when comparing vectors. Let’s look at the two plots below, showing word frequencies of Author A from her 1000-word text against Author B’s word frequencies, from a smaller and a larger text:
</p>
<p>
  <img src="/files/dnarr/auth01-fig05.png" alt="." width="502" height="365" />
</p>
<p>
  <img src="/files/dnarr/auth01-fig06.png" alt="." width="502" height="365" />
</p>
<p>
  Even though both graphs show the “raw” counts, you can see that one thing stays constant: the angle of the two arrows. That angle is precisely what we’re after.
</p>

<h2>Cosine similarity and higher dimensions</h2>
<p>
  For two-dimensional vectors, the <i>cosine similarity formula</i> gives us the cosine of the two vectors’ angle:
</p>
<p class="formula">
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>s</mi>
    <mo>=</mo>
    <mfrac>
      <mrow>
        <mfenced>
          <mrow>
            <msub>
              <mi>a</mi>
              <mi>x</mi>
            </msub>
            <mo>×</mo>
            <msub>
              <mi>b</mi>
              <mi>x</mi>
            </msub>
          </mrow>
        </mfenced>
        <mo>+</mo>
        <mfenced>
          <mrow>
            <msub>
              <mi>a</mi>
              <mi>y</mi>
            </msub>
            <mo>×</mo>
            <msub>
              <mi>b</mi>
              <mi>y</mi>
            </msub>
          </mrow>
        </mfenced>
      </mrow>
      <mrow>
        <msqrt>
          <mrow>
            <msup>
              <msub>
                <mi>a</mi>
                <mi>x</mi>
              </msub>
              <mn>2</mn>
            </msup>
            <mo>+</mo>
            <msup>
              <msub>
                <mi>a</mi>
                <mi>y</mi>
              </msub>
              <mn>2</mn>
            </msup>
          </mrow>
        </msqrt>
        <mo>×</mo>
        <msqrt>
          <mrow>
            <msup>
              <msub>
                <mi>b</mi>
                <mi>x</mi>
              </msub>
              <mn>2</mn>
            </msup>
            <mo>+</mo>
            <msup>
              <msub>
                <mi>b</mi>
                <mi>y</mi>
              </msub>
              <mn>2</mn>
            </msup>
          </mrow>
        </msqrt>
      </mrow>
    </mfrac>
  </math>
</p>
<p>
  In plain words, that means:
</p>
<ul>
  <li>
    If the two arrows point in the same direction, then <i>s</i> equals 1. That’s as similar as you can get.
  </li>
  <li>
    If the two arrows point in the opposite direction, then <i>s</i> equals -1. That’s as different as you can get.
  </li>
  <li>
    If the two arrows are at right angles, then <i>s</i> equals 0.
  </li>
  <li>
    The length of the arrows doesn’t matter.
  </li>
</ul>
<p>
  In all of the examples above, the cosine similarity of the two authors’ word frequency vectors
  (in our limited two-dimensional space) is approximately <i>s=0.97788</i>.
</p>
<p>
  There is only one hyperspace jump left to do, and then the arithmetic will end. The rest of authorial analysis is a mixture of  common sense and black magic. We don’t want to randomly select only two values (the frequency of two- and three-letter words) but work instead with our full 32-dimensional data in all its richness. But how do you imagine the angle between two 32-dimensional vectors?
</p>
<p>
  The trick is: you don’t. You just mechanically extend the formula, feed in the numbers you’ve observed, and walk away with the results.
</p>
<p>
  The cosine similarity formula above has the two dimensions hard-wired: it
  refers to <i>a<sub>x</sub></i> and <i>a<sub>y</sub></i>, and to <i>b<sub>x</sub></i> and <i>b<sub>y</sub></i>.
  If instead you write <i>a<sub>1</sub></i>, <i>a<sub>2</sub></i>, <i>b<sub>1</sub></i> and <i>b<sub>2</sub></i> and
  throw in a ∑ to represent adding up stuff, it can be rewritten as:
</p>
<p class="formula">
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>s</mi>
    <mo>=</mo>
    <mfrac>
      <mrow>
        <munderover>
          <mo>&Sum;</mo>
          <mrow>
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow>
            <mn>2</mn>
          </mrow>
        </munderover>
        <msub>
          <mi>a</mi>
          <mi>i</mi>
        </msub>
        <mo>×</mo>
        <msub>
          <mi>b</mi>
          <mi>i</mi>
        </msub>
      </mrow>
      <mrow>
        <msqrt>
          <mrow>
            <munderover>
              <mo>&Sum;</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mrow>
                <mn>2</mn>
              </mrow>
            </munderover>
            <msup>
              <msub>
                <mi>a</mi>
                <mi>i</mi>
              </msub>
              <mn>2</mn>
            </msup>
          </mrow>
        </msqrt>
        <mo>×</mo>
        <msqrt>
          <mrow>
            <munderover>
              <mo>&Sum;</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mrow>
                <mn>2</mn>
              </mrow>
            </munderover>
            <msup>
              <msub>
                <mi>b</mi>
                <mi>i</mi>
              </msub>
              <mn>2</mn>
            </msup>
          </mrow>
        </msqrt>
      </mrow>
    </mfrac>
  </math>
</p>
<p>
  That’s for 2 dimensions. If you need, say, n dimensions, then just do a bit of more adding up:
</p>

<p class="formula">
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>s</mi>
    <mo>=</mo>
    <mfrac>
      <mrow>
        <munderover>
          <mo>&Sum;</mo>
          <mrow>
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow>
            <mn>n</mn>
          </mrow>
        </munderover>
        <msub>
          <mi>a</mi>
          <mi>i</mi>
        </msub>
        <mo>×</mo>
        <msub>
          <mi>b</mi>
          <mi>i</mi>
        </msub>
      </mrow>
      <mrow>
        <msqrt>
          <mrow>
            <munderover>
              <mo>&Sum;</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mrow>
                <mn>n</mn>
              </mrow>
            </munderover>
            <msup>
              <msub>
                <mi>a</mi>
                <mi>i</mi>
              </msub>
              <mn>2</mn>
            </msup>
          </mrow>
        </msqrt>
        <mo>×</mo>
        <msqrt>
          <mrow>
            <munderover>
              <mo>&Sum;</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mrow>
                <mn>n</mn>
              </mrow>
            </munderover>
            <msup>
              <msub>
                <mi>b</mi>
                <mi>i</mi>
              </msub>
              <mn>2</mn>
            </msup>
          </mrow>
        </msqrt>
      </mrow>
    </mfrac>
  </math>
</p>
<p>That’s cosine similarity for a pair of n-dimensional vectors. Welcome to Alpha Centauri.</p>

<h2>OK, that was word length. What else?</h2>
<p>
  That’s as complicated as the math gets in the whole experiment, and if you take a closer look, you can see it doesn’t go beyond adding and multiplying numbers and calculating square roots – we’re just doing a whole lot of that.
</p>
<p>
  What really matters is the actual numbers we are crunching. So far I’ve kept referring to one type of data: the count (or frequency) of words that are 1, 2, 3 etc. letters long, up to 32. In his Language Log post, Patrick Juola described three additional linguistic variables that he worked with:
</p>
<ul>
  <li>
    The 100 most common words: what percentage of the document were “the,” what were “of,” and so on.
  </li>
  <li>
    The distribution of character 4-grams, groups of four adjacent characters.
  </li>
  <li>
    Finally, word bigrams, pairs of adjacent words.
  </li>
</ul>
<p>
  Why do these variables seem to help identify authorship and not others? In short, that’s what I call the “black magic” of statistical methods.
</p>
<p>
  How do these same variables fare for the authorial analysis of Hungarian texts? Are there other variables that may be worth
  looking at? And how do you know what works and what does not? To find the answers, you need to put black magic to the
  test of common sense. That’s what I’ll do in the next post .
</p>

    </div>
    <div class="push">&nbsp;</div>
  </div>
  <div class="footer-wrap">
    <div class="footer">&copy;2017 Gábor L Ugray &bull; <a href="/imprint">Imprint</a></div>
  </div>
  <script src='/_lib/jquery-3.1.1.min.js'></script>
<script src='/_lib/jquery.tooltipster.min.js'></script>
<script src='/_dev/jealous.js'></script>
</body>
</html>
