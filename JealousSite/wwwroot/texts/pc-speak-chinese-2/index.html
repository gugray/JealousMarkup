<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>PC speak Chinese #2 - Jealous Markup</title>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,600,700|Bree+Serif&amp;subset=latin-ext" rel="stylesheet">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel='stylesheet' href='/static/jealous.min.css?v=2BH27NGZPV7T3FBI4QX2DA5TBA'>
</head>
<body>
  <div class="content-wrap">
    <div class="header-wrap">
      <div class="header">
        <a href="/"><img src="/static/jealous-markup-white.svg" alt="jealous markup" /></a>
        <div class="subhead">
          <div class="tagline">Software and language, mostly</div>
          <div class="links">
            <a href="/projects">Projects</a>
            <span> &bull; </span>
            <a href="https://twitter.com/twilliability" target="_blank" rel="nofollow noreferrer">Twitter</a>
            <span> &bull; </span>
            <a href="https://github.com/gugray" target="_blank" rel="nofollow noreferrer">Github</a>
          </div>
        </div>
      </div>
    </div>
    <div class="content">

      

<h1>PC speak Chinese #2</h1>
<p class="meta"><span class='date'>April 2, 2015</span>
<span class='filedunder'>
<span>Software</span>
<span>Chinese</span>
</span>
</p>
<p class="lede">This is a follow-up to a previous <a href="http://blog.zydeo.net/pc-speak-chinese/" title="PC speak Chinese">collection of online resources</a> that you need if you are to create Chinese language processing software – e.g., a new dictionary :) The previous post was about fonts, dictionaries and example sentences; read on for more esoteric stuff such as character recognition, frequency lists and stroke order animations here.</p>

<h2>Character recognition</h2>
<p>
  An electronic Chinese dictionary with no ability to recognize
  handwritten characters makes about zero sense. Sure, generations have
  managed to learn Mandarin in the pre-digital age, but the sheer amount
  of time wasted navigating radical tables, counting strokes and turning
  pages makes me cringe. So the very first thing I looked at when I was
  considering whether Zydeo was feasible or not was the availability
  open-source handwriting recognition for Hanzi. These resources were hard
  to find, but indeed there are several alternatives.
</p>
<ul>
  <li>
    <b>Tegaki:</b><br>
    <a href="http://tegaki.org/" target="_blank">http://tegaki.org/</a><br>
    I believe this was the first project I came across. The video at the top
    of the page was very encouraging, and better still, there is a Windows
    installer too so I could try it easily. My spirits sank, however, when I
    saw it was in Python, which I am completely not familiar with, and my
    impressions of the actual character recognition performance were not
    very impressive either. (I must admit that I didn’t test extensively.)
    The copyright at the bottom of the page says 2009-2010, so it’s probably
    not an actively maintained project.<br>
    <br>
  </li>
  <li>
    <b>Tomoe:</b><br>
    <a href="http://tomoe.sourceforge.jp/cgi-bin/en/blog/index.rb" target="_blank">http://tomoe.sourceforge.jp/cgi-bin/en/blog/index.rb</a><br>
    The Tegaki website points to Tomoe as a “related” link, without
    specifying the connection between the two projects. Back last summer the
    most recent update on the site was from January 2014, and sadly it
    seemed there was only stroke order data for Kanji, not for simplified or
    traditional Chinese. The most recent news item, however, is from March
    31, 2015, and it announces nothing else but the addition of simplified
    Chinese. The project seems to be related to Red Hat, and I find the
    source code somewhat more accessible than Python, but it’s still not
    exactly up my alley. Being a non-Linux guy I didn’t get around to
    compiling and trying it, so I have no impressions about Tomoe’s quality.<br>
    <br>
  </li>
  <li>
    <b>Zinnia:</b><br>
    <a href="http://taku910.github.io/zinnia/" target="_blank">http://taku910.github.io/zinnia/</a><br>
    Both Tomoe and Tegaki refer to Zinnia, and whether or not they actually
    build on this code, it’s clear that both use the stroke order data that
    comes from Zinnia – the two packages include the exact same XML file for
    simplified Chinese, identical to the one that’s part of the Zinnia
    download. There is very little context on the website about the
    project’s history; some details on the page suggest that it was created
    in 2008.<br>
    <br>
  </li>
  <li>
    <b>Jordan Kiang’s HanziLookup:</b><br>
    <a href="http://www.kiang.org/jordan/software/hanzilookup/" target="_blank">http://www.kiang.org/jordan/software/hanzilookup/</a><br>
    This, finally, is where I hit gold! The author modestly writes that “it
    can’t really compete with commercial handwriting recognition, but hey,
    it’s just a widget I coded up in some spare time.” My experience,
    however, is very positive, and it turns out many or most other free
    dictionaries out there use HanziLookup too. It’s very straightforward
    Java code that I could port painlessly to C#. (Only… using AWT’s Bezier
    curve functions for solving 3rd degree polynomials was a <i>very nasty</i> move, Jordan. But more on that later, in a different post.)<br>
    I am apparently not the only one porting HanziLookup to something else; <a href="http://www.joshuakoo.com/" target="_blank">Joshua Koo</a> from Singapore built a JS widget based on it in 2010; more at <a href="http://www.lab4games.net/zz85/blog/2010/02/17/js-%E4%B8%AD%E6%96%87%E7%AC%94%E7%94%BB%E8%BE%93%E5%85%A5%E6%B3%95-javascript-chinese-stroke-input/" target="_blank">JS 中文笔画输入法</a>.<br>
    <br>
  </li>
</ul>
<h2>Frequency lists</h2>
<p>
  Knowing what words are frequent and which ones are rare is not
  absolutely necessary to create a good dictionary application, but it can
  be useful. CC-CEDICT is very much a Chinese to English dictionary (it
  is structured around Chinese headwords), but you can also search for
  English words that occur in the definitions. Now if you’re searching
  for, say, “tree,” it would be ideal if Zydeo listed the more common
  Chinese words for “tree” first, instead of following some random order.
  This is not happening in Zydeo’s first version yet, but it’s one of the
  more obvious improvements down the road.
</p>
<p>
  Here’s what I found in terms of Chinese word frequency lists. Note that coming up with <i>word</i>
  frequencies is by no means a trivial task since the script does not
  indicate word boundaries with spaces, and in fact there is often little
  agreement about what exactly constitutes a word in Chinese. I completely
  ignore this can of worms for now, and just share what I have found.
</p>
<ul>
  <li>
    <b>Modern Chinese Character Frequency List</b> from MTSU:<br>
    <a href="http://lingua.mtsu.edu/chinese-computing/statistics/char/list.php?Which=MO" target="_blank">http://lingua.mtsu.edu/chinese-computing/statistics/char/list.php?Which=MO</a><br>
    As the title says, this is a <i>character</i> frequency list, which
    falls short of what we need. It is also a relatively random choice;
    finding Chinese character frequency lists on the Internet is not that
    difficult. I’m including it here nonetheless mostly for nostalgic
    reasons, because this is one of the first resources I came across during
    my quest.<br>
    <br>
  </li>
  <li>
    <b>Word frequencies from the University of Leeds:</b><br>
    <a href="http://corpus.leeds.ac.uk/frqc/lcmc.num" target="_blank">http://corpus.leeds.ac.uk/frqc/lcmc.num</a><br>
    <a href="http://corpus.leeds.ac.uk/frqc/internet-zh.num" target="_blank">http://corpus.leeds.ac.uk/frqc/internet-zh.num</a><br>
    These lists are from two separate Chinese corpora from the University of Leeds. They used <a href="http://www.nlplab.cn/" target="_blank">NuiTrans</a>,
    nothing less than a statistical machine translation system, to segment
    the text (identify words and parts of speech), and although I did not
    find a way to access the two corpora in their entirety, the two links
    above provide just what I need: word frequency lists.<br>
    <br>
  </li>
  <li>
    <b>A Review of Chinese Word Lists Accessible on the Internet:</b><br>
    <a href="http://technology.chtsai.org/wordlist/" target="_blank">http://technology.chtsai.org/wordlist/</a><br>
    An overview that does not itself contain word lists, but points to very useful sources.<br>
    <br>
  </li>
  <li>
    <b>Mandarin Frequency lists from Wiktionary:</b><br>
    <a href="http://en.wiktionary.org/wiki/Appendix:Mandarin_Frequency_lists" target="_blank">http://en.wiktionary.org/wiki/Appendix:Mandarin_Frequency_lists</a><br>
    Claims to contain the top 20k most frequent Mandarin words, based on the
    work of K. J. Chen and the CKIP Group of the Academica Sinica, but
    unfortunately there’s no information on exactly how the list was
    created. In good Wiktionary fashion there is no single text file to
    download, but instead 20 Wiki pages to browse and copy-paste from. I
    also have my doubts about the reliability of this list because it
    contains countless duplicates, which is not indicative of good data
    hygiene. In any case, the Wiktionary team apparently relies on it to
    prioritize their work, so I lend it credence as one of many sources.<br>
    <br>
  </li>
  <li>
    <b>SUBTLEX-CH: Chinese Word and Character Frequencies Based on Film Subtitles:</b><br>
    <a href="http://expsy.ugent.be/subtlex-ch/" target="_blank">http://expsy.ugent.be/subtlex-ch/</a><br>
    This is by far the most convincing word frequency list that I found. The authors provide a proper reference (<b>Cai, Q., &amp; Brysbaert, M.</b> (2010). SUBTLEX-CH: Chinese Word and Character Frequencies Based on Film Subtitles. <i>Plos ONE, 5(6), e10729.</i>) and <a href="http://expsy.ugent.be/subtlex-ch/Cai%20&amp;%20Brysbaert%202010%20Plos%20One.pdf" target="_blank">link to their paper</a> directly. As they write about their methodology, <i>
      In
      line with what has been found in the other languages, the new word and
      character frequencies explain significantly more of the variance in
      Chinese word naming and lexical decision performance than measures based
      on written texts.
    </i> In other words, film subtitles reflect everyday
    language better than so many newspaper articles, novels or Wikipedia
    pages. That’s my kind of talk.<br>
    <br>
  </li>
  <li>
    <b>HSK word lists:</b><br>
    <a href="http://www.hskhsk.com/word-lists.html" target="_blank">http://www.hskhsk.com/word-lists.html</a><br>
    汉语水平考试 (aka HSK) is the official test of Chinese competence in the PRC,
    and it comes with six lists that specify what words a student is
    expected to know to pass each of the six levels. That is pretty much the
    opposite way of approaching the reality of language from what
    SUBTLEX-CH does, and sure enough, you’ll find lots of words in the
    colloquial top 1000 that don’t even make it to HSK 6, and conversely,
    the HSK lists are guaranteed to contain a few oddballs that no living
    Chinese would use under normal circumstances. But HSK <i>is</i> the exam many students of Chinese aim for, and the HSK level is one measure of a word’s frequency in its own peculiar way.<br>
    <br>
  </li>
</ul>
<h2>Stroke order animations</h2>
<p>
  If you’re at a somewhat advanced stage with Chinese, then by looking
  at an unknown character you are supposed to know the correct stroke
  order. That is the theory at least – with 1500 Hanzi under my belt, I
  still keep getting surprised regularly at one or the other character. So
  while stroke order animations are not such an absolute must-have in a
  dictionary tool as character recognition, I see them as a big value add.
</p>
<p>
  Are there freely available character animations out there? Here the
  answer is rather negative. A number of sites provide them for free,
  either as GIFs or as Flash animations, but apparently the animations are
  mostly syndicated from a few IP owners in exchange for ad revenues. Not
  a workable solution for an offline tool.
</p>
<p>
  I would personally find it very exciting to launch an open-source
  stroke order animation project, but even after a little thought given to
  how this could be achieved, it turns out it’s just a bit more
  complicated than creating a brand new Chinese font. Read: it’s a LOT of
  work. This is worth a series of posts on its own; for now, here’s what
  I’ve found out there so far.
</p>
<ul>
  <li>
    <b>It gets tough.</b><br>
    After countless hours of Googling, the first sign of hope came when I
    found this age-old discussion (from 2011) on Google Groups: <a href="https://groups.google.com/forum/#%21topic/kanjivg/lqj5O3eahC4" target="_blank">KanjiVG internationalization (simplified chinese)</a>.
    It’s dives right into deep waters discussing Hanzi fonts in vector
    graphics and character recognition projects, but it contains priceless
    pointers to people working on such stuff, including animated stroke
    order fonts. Grab a cup of coffee and read the whole thing.<br>
    <br>
  </li>
  <li>
    <b>Wikimedia Commons Stroke Order Project:</b><br>
    <a href="http://commons.wikimedia.org/wiki/Commons:Stroke_Order_Project" target="_blank">http://commons.wikimedia.org/wiki/Commons:Stroke_Order_Project</a><br>
    Yay! Exactly the kind of thing I’m looking for. Stroke-order animations,
    open-source, community edited, in a vector graphics format. Except… I
    cannot find such basic information as: exactly what characters are
    covered? Where is the download link? Lots of content scattered across so
    many pages, but with so much effort I am still somehow unable to make
    head or tail of it. The opening page was last updated in October 2013…
    Is this project alive? Are the individual characters being hand-drawn,
    or is there some degree of technology behind it? Who are the people
    behind this, and how can you get in touch with them? These are all
    genuine questions, not a sneaky vote of non-confidence, but for
    practical purposes I had to conclude this is not (yet) what I’m looking
    for, after all.<br>
    <br>
  </li>
  <li>
    <b>Chinese Stroke Order Font</b><br>
    <a href="http://rtega.be/chmn/index.php?subpage=68" target="_blank">http://rtega.be/chmn/index.php?subpage=68</a><br>
    Created by Reinaert Albrecht, this is a “regular” font derived from the
    open-source Arphic font, but it’s not about animation: it contains tiny
    numbers written next to each stroke to indicate their order. The problem
    with both OTF and TTF fonts is that once they’re compiled, they contain
    only the outline, and no hint about individual strokes. This font adds
    extra detail (the tiny numbers indicating the order), but does not
    decompose characters into their strokes, so it won’t do as the basis of
    stroke animations. It also covers relatively few characters – those in
    HSK levels 1 through 3.<br>
    <br>
  </li>
  <li>
    <b>GlyphWiki:</b><br>
    <a href="http://en.glyphwiki.org/wiki/GlyphWiki:MainPage" target="_blank">http://en.glyphwiki.org/wiki/GlyphWiki:MainPage</a><br>
    Erhm… This is a site about – glyphs. As the starting page says,
    “GlyphWiki is a shareable kanji (hanzi, hanja) glyph database system on
    the Internet for a solution of using unencoded kanji characters in
    digitizing text resources in kanji.” You can indeed search for
    characters over the web interface and find SVG representations and a lot
    of descriptive information about them, but there is no single,
    structured data set, and no clear decomposition into strokes.<br>
    <br>
  </li>
  <li>
    <b>Chinese character description languages:</b><br>
    <a href="http://en.wikipedia.org/wiki/Chinese_character_description_languages" target="_blank">http://en.wikipedia.org/wiki/Chinese_character_description_languages</a><br>
    By this point in my research I have resigned myself to the thought that
    an open-source, vector graphics based database of CJK ideographs is
    something that waits to be created – and that surely involves coming up
    with a representation so you only need to deal with recurring components
    once. That’s how I found this Wikipedia page about character
    description languages. It is by all means an engaging read, but my hopes
    of finding publicly available resources based on any of these
    description languages did not come true. But the <i>External links</i> section pointed me to the next step: Wenlin.<br>
    <br>
  </li>
  <li>
    <b>CDL – An XML application for rendering and indexing Han (CJKV) characters:</b><br>
    <a href="http://www.wenlin.com/cdl/" target="_blank">http://www.wenlin.com/cdl/</a><br>
    Once again, hope shimmered: the sheer amount of content on this page
    shows these guys know what they are talking about. A closer look at
    their work also makes it clear that they are not only talking to
    themselves: they have been actively contributing to Unicode / Unihan and
    the W3C. It is also apparently the case that they are not giving away
    their data for free; this is a company that makes its livelihood by
    selling its IP. If Zydeo were a commercial project, I would talk to them
    without hesitation, but as things stand, Wenlin is not the end of my
    search.<br>
    <br>
  </li>
  <li>
    <b>CJK Decomposition Data</b><br>
    <a href="http://cjkdecomp.codeplex.com/wikipage?title=cjk-decomp" target="_blank">http://cjkdecomp.codeplex.com/wikipage?title=cjk-decomp</a><br>
    This project by <a href="http://www.codeplex.com/site/users/view/gavingrover" target="_blank">gavingrover</a> is without doubt the single most impressive one I have come across during the entire “stroke order” journey. Gavin, <i>after working in IT for 15yrs, switched to teaching English, while learning Chinese on the side</i>
    – and created a decomposition of 75k Chinese/Japanese characters into
    their components. Unicode also makes a half-hearted attempt at
    representing CJK ideographs as combinations of their components, but the
    kinds of combinations it supports do not seem to be enough, and the
    components themselves need to be characters with a Unicode code point.
    The decomposition data here goes beyond that level; I get the shivers
    when I think of the amount of work this must have taken “while learning
    Chinese on the side.” If I do embark on creating a stroke order
    animation font from scratch (or any CJK font for that matter), then this
    data set will definitely be my starting point.<br>
    <br>
  </li>
</ul>
<h2>Offshoot: fonts</h2>
<p>
  While I was trying to chase up that perfect source for character
  animations, I came across two really interesting papers; in fact, both
  are theses of computer science graduates
</p>
<p>
  <em><a href="http://blog.foolip.org/2008/06/12/vector-graphics-stylized-stroke-fonts" target="_blank">Philip Jägenstedt (2008): Vector Graphics Stylized Stroke Fonts</a></em> (referenced in a <a href="http://www.diva-portal.org/smash/get/diva2:18417/FULLTEXT01.pdf" target="_blank">blog post</a> by him) looks at efficient ways of representing CJK ideographs in fonts. <em><a href="http://www.eecs.tufts.edu/%7Ejakubiak/downloads/Thesis.pdf" target="_blank">Elena Jakubiak Hutchinson (2009): An Improved Representation for Stroke-Based Typefaces and a Method for their Creation</a></em>
  looks at how stroke-based characters can be represented in a modular
  typeface. Both texts contain an exciting introduction into the inner
  workings of modern fonts as well as a wealth of information about the
  CJK script itself.
</p>
<p>
  Finally, it was inevitable that I came across the great free font editor <a href="http://fontforge.github.io/en-US/" target="_blank">FontForge</a>,
  a piece of software that allows you to open any TTF file and view or
  even edit the outline of characters in there – in other words, to play
  typographer in your spare time and create your own fonts.
</p>

    </div>
    <div class="push">&nbsp;</div>
  </div>
  <div class="footer-wrap">
    <div class="footer">&copy;2017 Gábor L Ugray &bull; <a href="/imprint">Imprint</a></div>
  </div>
  <script src='/static/jealous.min.js?v=X2TWUUIMLBCBAIUIFA2QXR4JZI'></script>

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-69170449-4', 'auto');
  ga('send', 'pageview');
  </script>
</body>
</html>
